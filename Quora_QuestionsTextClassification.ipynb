{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01f361ddc47e0b386595316fe3d7f4dabbd260db",
    "id": "2mEeooLAcN3T"
   },
   "source": [
    "**Notebook Objective:**\n",
    "\n",
    "Objective of the notebook is to look at the different pretrained embeddings provided in the dataset and to see how they are useful in the model building process. \n",
    "\n",
    "First let us import the necessary modules and read the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z35l5ysZdD9Z"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4DOLuYqDdyrP"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "Bvw9DLdXcN3d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,  Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.python.keras.layers import CuDNNGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXjoSv3PcN4x",
    "outputId": "6fae16a1-c2b7-4038-eec4-985f109cd860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/content/sample_data/train.csv\")\n",
    "print(\"Train shape : \",train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "m1IuSDy7ca5R",
    "outputId": "8d94ff93-d855-4eb2-cee2-9be950ad4cf0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1225312</td>\n",
       "      <td>1225312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80810</td>\n",
       "      <td>80810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            qid  question_text\n",
       "target                        \n",
       "0       1225312        1225312\n",
       "1         80810          80810"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_types = train_df.groupby('target').agg('count')\n",
    "target_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_MK2lqTCca2O"
   },
   "outputs": [],
   "source": [
    "target_labels = train_df.target.sort_values().index\n",
    "target_counts = train_df.target.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7gao6ZyRgXgb"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jr4SGuUlgXmu"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqin-X4OgXzJ",
    "outputId": "933b05b5-f407-4310-9786-1d92a0ac5154"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.remove('not') #remove not from the words as it is negative\n",
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R-Tva5mYgXdj"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KJJD_77VoLS9"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(questions):\n",
    "    #data cleaning\n",
    "    questions = re.sub(re.compile('<.*?>'),'',questions)\n",
    "    questions = re.sub('[^A-Za-z0-9]+',' ',questions)\n",
    "    \n",
    "    #Lowercase\n",
    "    questions = questions.lower()\n",
    "\n",
    "    #tokenization\n",
    "    tokens = nltk.word_tokenize(questions)\n",
    "\n",
    "    #stop words removal\n",
    "    questions = [word for word in tokens if word not in eng_stopwords] #remove stop wprds\n",
    "\n",
    "    #lemmatization\n",
    "    questions = [lemmatizer.lemmatize(word) for word in questions]\n",
    "\n",
    "    #join words in preprocessed questions\n",
    "    questions = ' '.join(questions)\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "GyklJoDEoLiC",
    "outputId": "53d35972-8b93-46d4-ba73-b9f91fc82a8e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>preprocessed_question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>quebec nationalist see province nation 1960s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>adopted dog would encourage people adopt not shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>velocity affect time velocity affect space geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "      <td>otto von guericke used magdeburg hemisphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "      <td>convert montra helicon mountain bike changing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...                         preprocessed_question_text\n",
       "0  00002165364db923c7e6  ...       quebec nationalist see province nation 1960s\n",
       "1  000032939017120e6e44  ...  adopted dog would encourage people adopt not shop\n",
       "2  0000412ca6e4628ce2cf  ...  velocity affect time velocity affect space geo...\n",
       "3  000042bf85aa498cd78e  ...        otto von guericke used magdeburg hemisphere\n",
       "4  0000455dfa3e01eae3af  ...  convert montra helicon mountain bike changing ...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['preprocessed_question_text']=train_df[\"question_text\"].apply(lambda question_text: data_preprocessing(question_text))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PwT5oLlwoLtn"
   },
   "outputs": [],
   "source": [
    "## split to train and val\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JvbXrrf8yf7B"
   },
   "outputs": [],
   "source": [
    "## fill up the missing values in the question_text with \"_na_\"\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "val_df[\"question_text\"] = val_df[\"question_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ludfkTbzyMzF"
   },
   "outputs": [],
   "source": [
    "## Get the target values\n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy0YA6HJvSFW"
   },
   "source": [
    "Bag of Words:It converts a collection of text documents to a matrix of token counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AjtGnIEpuyeJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZbUFeUjluyzo"
   },
   "outputs": [],
   "source": [
    "vect= CountVectorizer(dtype=np.float32,strip_accents='unicode',\n",
    "                      analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1,3), min_df = 3)\n",
    "X_train = vect.fit_transform(list(train_df['preprocessed_question_text'].values))\n",
    "X_val = vect.transform(val_df['preprocessed_question_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0Lhy3zsP0A4w"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c66nm8mD1BO8",
    "outputId": "68e9ff42-6725-4340-d65f-6fac2a311976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=MultinomialNB()\n",
    "clf.fit(X_train,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ugt6DkaO1PmW",
    "outputId": "323bc11b-6c16-4b7c-a249-8249f9f6cbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.9270640598003762\n",
      "Validation f1_score:  0.5412606943931684\n"
     ]
    }
   ],
   "source": [
    "y_val = clf.predict(X_val)\n",
    "print(\"Validation accuracy: \",accuracy_score(val_y,y_val))\n",
    "print(\"Validation f1_score: \",f1_score(val_y,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JAhAldYYQ2yU"
   },
   "outputs": [],
   "source": [
    "del X_train,vect,X_val\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_Z2-MuI18hJ"
   },
   "source": [
    "TFIDF(Term Frequency Inverse Document Frequency): It shows how important a word is to a document in a collection or corpus.\n",
    "The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word.\n",
    "\n",
    "Sentence 1: The car is driven on the road.\n",
    "Sentence 2: The truck is driven on the highway.\n",
    "Here TF-IDF of the above 2 documents , represent our corpus.\n",
    "'The'--> TF--> A --> 1/7, B--> 1/7, IDF = log(2/2)=0 , TF-IDF --> A -> 0, B-> 0 \n",
    "Similarly for the words 'is','driven','on','the'.\n",
    "But for the words 'car' and 'truck' have more significance.\n",
    "\n",
    "'car'--> TF--> A -->1/7,B --> 0/7,IDF = log(2/1)=0.3,TF-IDF --> A-> 0.043,B-> 0\n",
    "\n",
    "'truck'--> TF-->A-->0/7,B --> 1/7,IDF = log(2/1)=0.3,TF-IDF --> A-> 0,B-> 0.043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HwToEMms2yyt"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "voJispvP2-xr"
   },
   "outputs": [],
   "source": [
    "tfidfvec= TfidfVectorizer(dtype=np.float32,strip_accents='unicode',\n",
    "                      analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1,3), min_df = 3,\n",
    "                      max_features=None,use_idf=1,smooth_idf=1,sublinear_tf=1,stop_words='english')\n",
    "X_train_tfidf = tfidfvec.fit_transform(list(train_df['preprocessed_question_text'].values) )\n",
    "X_val_tfidf = tfidfvec.transform(val_df['preprocessed_question_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGC1no932--t",
    "outputId": "bcc45034-6e40-4a21-d311-44a11729c023"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=BernoulliNB()\n",
    "clf.fit(X_train_tfidf,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fe_cpcrZ_SSN",
    "outputId": "d680b458-7431-40ce-cba6-2abc75cd5396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.9379461357656372\n",
      "Validation f1_score:  0.5113643214565623\n"
     ]
    }
   ],
   "source": [
    "y_val = clf.predict(X_val_tfidf)\n",
    "print(\"Validation accuracy: \",accuracy_score(val_y,y_val))\n",
    "print(\"Validation f1_score: \",f1_score(val_y,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gcfhPiAgREgG"
   },
   "outputs": [],
   "source": [
    "del X_train_tfidf,tfidfvec,X_val_tfidf\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVYMuTo_BB50"
   },
   "source": [
    "HashingVectorizer: The HashingVectorizer is based on feature hashing and is a memory efficient technique, also known as the hashing trick. Unlike the CountVectorizer where the index assigned to a word in the document vector is determined by the alphabetical order of the word in the vocabulary, the HashingVectorizer maintains no vocabulary and determines the index of a word in an array of fixed size via hashing(So no worry of mis-spelling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ozcUWvHFAuXV"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rJ-yDKIZAui4"
   },
   "outputs": [],
   "source": [
    "hashvec= HashingVectorizer(dtype=np.float32,strip_accents='unicode',\n",
    "                      analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                      ngram_range=(1,3),n_features = 2**10)\n",
    "X_train_hashvec = hashvec.fit_transform(list(train_df['preprocessed_question_text'].values))\n",
    "X_val_hashvec = hashvec.transform(val_df['preprocessed_question_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufbn5TsgAuoX",
    "outputId": "eeb9dd65-5da4-496f-f5ea-9a006bfcc487"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=GaussianNB()\n",
    "clf.fit(X_train_hashvec.toarray(),train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ouj6ayAzAu0Y",
    "outputId": "4b304af5-1c47-4866-b3e0-a4cd7e6a4bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.7236019058945429\n",
      "Validation f1_score:  0.22872647253615908\n"
     ]
    }
   ],
   "source": [
    "y_val = clf.predict(X_val_hashvec.toarray())\n",
    "print(\"Validation accuracy: \",accuracy_score(val_y,y_val))\n",
    "print(\"Validation f1_score: \",f1_score(val_y,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nGvCHzaAu-h",
    "outputId": "817673cf-4e29-4c65-af74-498cb788312b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=BernoulliNB()\n",
    "clf.fit(X_train_hashvec,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgjVxWFjJxk4",
    "outputId": "f37ec3de-890e-46f8-dab9-81500ba63539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.8969163197962418\n",
      "Validation f1_score:  0.26971614536250227\n"
     ]
    }
   ],
   "source": [
    "y_val = clf.predict(X_val_hashvec)\n",
    "print(\"Validation accuracy: \",accuracy_score(val_y,y_val))\n",
    "print(\"Validation f1_score: \",f1_score(val_y,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HPiOJTSPRi7y"
   },
   "outputs": [],
   "source": [
    "del X_train_hashvec,hashvec,X_val_hashvec\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrIYz3aOIzSB"
   },
   "source": [
    "Next steps are as follows:\n",
    " * Split the training dataset into train and val sample. Cross validation is a time consuming process and so let us do simple train val split.\n",
    " * Fill up the missing values in the text column with '_na_'\n",
    " * Tokenize the text column and convert them to vector sequences\n",
    " * Pad the sequence as needed - if the number of words in the text is greater than 'max_len' trunacate them to 'max_len' or if the number of words in the text is lesser than 'max_len' add zeros for remaining values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX8URUYAK3Rh"
   },
   "source": [
    "Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dty0oeEm2y43"
   },
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_df[\"question_text\"]))\n",
    "train_X = tokenizer.texts_to_sequences(train_df[\"question_text\"])\n",
    "val_X = tokenizer.texts_to_sequences(val_df[\"question_text\"])\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a5f324273d8e4726a6f0f9206170845d5ead890",
    "id": "DYS-vhJRcN6G"
   },
   "source": [
    "**Without Pretrained Embeddings:**\n",
    "\n",
    "Now that we are done with all the necessary preprocessing steps, we can first train a Bidirectional GRU model. We will not use any pre-trained word embeddings for this model and the embeddings will be learnt from scratch. Please check out the model summary for the details of the layers used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3cfab26c6cced33ef7ab84f0d36997113131d530",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOkRBdX4cN6M",
    "outputId": "0676af85-0e2e-4cdd-a463-dc06359b4f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,142,625\n",
      "Trainable params: 15,142,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b71267b19ec4b8a4c977cb3ad87b636390e5cdb9",
    "id": "HXP1ztgkcN60"
   },
   "source": [
    "Train the model using train sample and monitor the metric on the valid sample. This is just a sample model running for 2 epochs. Changing the epochs, batch_size and model parameters might give us a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef1e1015e7c3ab5bc5d9774e49820c4b286d7847",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAqx-RVDcN64",
    "outputId": "18583a01-8df3-41a6-9278-4818b5848ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1786/1786 [==============================] - 392s 220ms/step - loss: 0.1254 - accuracy: 0.9527 - val_loss: 0.1092 - val_accuracy: 0.9560\n",
      "Epoch 2/2\n",
      "1786/1786 [==============================] - 384s 215ms/step - loss: 0.0982 - accuracy: 0.9608 - val_loss: 0.1108 - val_accuracy: 0.9569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f73aefe66a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the model \n",
    "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a72ba82481de9f96c62c334cc40bd1d134d38e2d",
    "id": "ORmuFrw1cN7a"
   },
   "source": [
    "Now let us get the validation sample predictions and also get the best threshold for F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47b63dca0247a08a808db7ae6eea33065c554948",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uW7GlkCycN7f",
    "outputId": "4c9d0591-a6dc-4fd9-bb5b-69269d253ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - 18s 47ms/step\n",
      "F1 score at threshold 0.1 is 0.5898567827847954\n",
      "F1 score at threshold 0.11 is 0.5974824006681779\n",
      "F1 score at threshold 0.12 is 0.6033436041367949\n",
      "F1 score at threshold 0.13 is 0.6099999999999999\n",
      "F1 score at threshold 0.14 is 0.6148175217273868\n",
      "F1 score at threshold 0.15 is 0.6200178296458385\n",
      "F1 score at threshold 0.16 is 0.6234161008408915\n",
      "F1 score at threshold 0.17 is 0.6266816708376023\n",
      "F1 score at threshold 0.18 is 0.6291448367663873\n",
      "F1 score at threshold 0.19 is 0.6315771212462917\n",
      "F1 score at threshold 0.2 is 0.6338647802062437\n",
      "F1 score at threshold 0.21 is 0.635869759462454\n",
      "F1 score at threshold 0.22 is 0.6375230986629951\n",
      "F1 score at threshold 0.23 is 0.6382963085460192\n",
      "F1 score at threshold 0.24 is 0.6392397987702627\n",
      "F1 score at threshold 0.25 is 0.6400573433432678\n",
      "F1 score at threshold 0.26 is 0.6394544829430416\n",
      "F1 score at threshold 0.27 is 0.6398794575590155\n",
      "F1 score at threshold 0.28 is 0.6398796733992265\n",
      "F1 score at threshold 0.29 is 0.6392681432890083\n",
      "F1 score at threshold 0.3 is 0.6383191395785854\n",
      "F1 score at threshold 0.31 is 0.6379407958589453\n",
      "F1 score at threshold 0.32 is 0.6372603075415059\n",
      "F1 score at threshold 0.33 is 0.6358777523267092\n",
      "F1 score at threshold 0.34 is 0.6342966602815023\n",
      "F1 score at threshold 0.35 is 0.633295459327526\n",
      "F1 score at threshold 0.36 is 0.6314425323985554\n",
      "F1 score at threshold 0.37 is 0.6298323566660228\n",
      "F1 score at threshold 0.38 is 0.627461184838234\n",
      "F1 score at threshold 0.39 is 0.6257193496860024\n",
      "F1 score at threshold 0.4 is 0.6225030935124625\n",
      "F1 score at threshold 0.41 is 0.6198420841325779\n",
      "F1 score at threshold 0.42 is 0.6171449164376773\n",
      "F1 score at threshold 0.43 is 0.6144679689809656\n",
      "F1 score at threshold 0.44 is 0.6108980688419942\n",
      "F1 score at threshold 0.45 is 0.6071188405797101\n",
      "F1 score at threshold 0.46 is 0.6042300313508961\n",
      "F1 score at threshold 0.47 is 0.6009775448041368\n",
      "F1 score at threshold 0.48 is 0.5978856136006476\n",
      "F1 score at threshold 0.49 is 0.5945764177455384\n",
      "F1 score at threshold 0.5 is 0.5906446921958313\n"
     ]
    }
   ],
   "source": [
    "pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3831ee610fee119c9851b5ca29b2d80b102ae6a",
    "id": "aSCLekLYcN9z"
   },
   "source": [
    "Now that our model building is done, it might be a good idea to clean up some memory before we go to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a36a071fb50f6c120e099b5fe27ad6ac977f1125",
    "id": "-LmGQ7MycN-B"
   },
   "outputs": [],
   "source": [
    "del model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKMnEkPfMrS0",
    "outputId": "240008d4-dce1-454b-d57b-b8c5f29925b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-16 18:36:39--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
      "--2020-11-16 18:36:39--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2176768927 (2.0G) [application/zip]\n",
      "Saving to: ‘glove.840B.300d.zip’\n",
      "\n",
      "glove.840B.300d.zip 100%[===================>]   2.03G  2.25MB/s    in 16m 54s \n",
      "\n",
      "2020-11-16 18:53:33 (2.05 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://nlp.stanford.edu/data/glove.840B.300d.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "717f7dcd5ccf71e83d0f062e221c46db39845e4d",
    "id": "fG9cFlkTcN_U"
   },
   "source": [
    "So we got some baseline GRU model without pre-trained embeddings. Now let us use the provided embeddings and rebuild the model again to see the performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c0010e518288bc7f588776c58610949140a139a",
    "id": "1Noxns6ucOAC"
   },
   "source": [
    "We have four different types of embeddings.\n",
    " * GoogleNews-vectors-negative300 - https://code.google.com/archive/p/word2vec/\n",
    " * glove.840B.300d - https://nlp.stanford.edu/projects/glove/\n",
    " * paragram_300_sl999 - https://cogcomp.org/page/resource_view/106\n",
    " * wiki-news-300d-1M - https://fasttext.cc/docs/en/english-vectors.html\n",
    " \n",
    " A very good explanation for different types of embeddings are given in this [kernel](https://www.kaggle.com/sbongo/do-pretrained-embeddings-give-you-the-extra-edge). Please refer the same for more details..\n",
    "\n",
    "**Glove Embeddings:**\n",
    "\n",
    "In this section, let us use the Glove embeddings and rebuild the GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ROX3PobNg1n",
    "outputId": "8e28a9bd-48cb-4aac-a3ea-983d8a4998ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.840B.300d.zip\n",
      "  inflating: glove.840B.300d.txt     \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXHliIpDNmel"
   },
   "outputs": [],
   "source": [
    "!rm glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "23f130e80159bb1701e449e2e91199dbfff1f1d4",
    "id": "DK-3KC5hcOAN"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): \n",
    "  return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qKgFGOwOlZE",
    "outputId": "fe96f850-3063-4dac-9d24-25f57ad72fd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jmcVn4MEdn25"
   },
   "outputs": [],
   "source": [
    "del all_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "K49hBHmIOwna"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2pHo4Q2Owjj",
    "outputId": "77da4760-bc51-44c2-8b58-7bb600e1ebf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,142,625\n",
      "Trainable params: 15,142,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "a560ab0dbab9cf6fdbdae6721ec030e300f19d78",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JU09BMMBcOA0",
    "outputId": "0840c79e-5c35-4053-af07-473b92bb7410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1786/1786 [==============================] - 390s 218ms/step - loss: 0.1138 - accuracy: 0.9557 - val_loss: 0.1029 - val_accuracy: 0.9583\n",
      "Epoch 2/2\n",
      "1786/1786 [==============================] - 387s 216ms/step - loss: 0.0938 - accuracy: 0.9625 - val_loss: 0.1016 - val_accuracy: 0.9591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f753c0ff6a0>"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "ff43855164472de035a5a1d80b3db4838684701a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNZTyWExcOBg",
    "outputId": "44094a13-0438-4de5-884a-2abe79416b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - 19s 49ms/step\n",
      "F1 score at threshold 0.1 is 0.5901340837609668\n",
      "F1 score at threshold 0.11 is 0.5983845730959529\n",
      "F1 score at threshold 0.12 is 0.6060832566697333\n",
      "F1 score at threshold 0.13 is 0.6131348357412888\n",
      "F1 score at threshold 0.14 is 0.6196678752070185\n",
      "F1 score at threshold 0.15 is 0.6252388317714493\n",
      "F1 score at threshold 0.16 is 0.6300078554595445\n",
      "F1 score at threshold 0.17 is 0.634832864729772\n",
      "F1 score at threshold 0.18 is 0.6393878908848969\n",
      "F1 score at threshold 0.19 is 0.6430908391070055\n",
      "F1 score at threshold 0.2 is 0.646455466112698\n",
      "F1 score at threshold 0.21 is 0.6493459600518636\n",
      "F1 score at threshold 0.22 is 0.6521500822163535\n",
      "F1 score at threshold 0.23 is 0.6546010106945587\n",
      "F1 score at threshold 0.24 is 0.6564763762829757\n",
      "F1 score at threshold 0.25 is 0.6584404455869751\n",
      "F1 score at threshold 0.26 is 0.6609505993210004\n",
      "F1 score at threshold 0.27 is 0.6628657374210812\n",
      "F1 score at threshold 0.28 is 0.6644775170806632\n",
      "F1 score at threshold 0.29 is 0.665692754449413\n",
      "F1 score at threshold 0.3 is 0.6666427108412089\n",
      "F1 score at threshold 0.31 is 0.6681159420289855\n",
      "F1 score at threshold 0.32 is 0.6684696208314298\n",
      "F1 score at threshold 0.33 is 0.6696747578179676\n",
      "F1 score at threshold 0.34 is 0.6704026721098534\n",
      "F1 score at threshold 0.35 is 0.6714817656194451\n",
      "F1 score at threshold 0.36 is 0.6721506329830387\n",
      "F1 score at threshold 0.37 is 0.6723659713514951\n",
      "F1 score at threshold 0.38 is 0.6723705617762464\n",
      "F1 score at threshold 0.39 is 0.6719509372830362\n",
      "F1 score at threshold 0.4 is 0.6717311054983485\n",
      "F1 score at threshold 0.41 is 0.6717841267354653\n",
      "F1 score at threshold 0.42 is 0.6713242549832248\n",
      "F1 score at threshold 0.43 is 0.6706455784342982\n",
      "F1 score at threshold 0.44 is 0.6695383875621094\n",
      "F1 score at threshold 0.45 is 0.6689535493764881\n",
      "F1 score at threshold 0.46 is 0.6679553177203085\n",
      "F1 score at threshold 0.47 is 0.667240990298033\n",
      "F1 score at threshold 0.48 is 0.6657565979978489\n",
      "F1 score at threshold 0.49 is 0.6643869891576314\n",
      "F1 score at threshold 0.5 is 0.6629546361859937\n"
     ]
    }
   ],
   "source": [
    "pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "fP4p8mxy-tss"
   },
   "outputs": [],
   "source": [
    "del word_index, embeddings_index, embedding_matrix, model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc6bab22dd12a09378f4b8b159cb7a5d88a3e7c0",
    "id": "bTUOdpqBcODF"
   },
   "source": [
    "**Wiki News FastText Embeddings:**\n",
    "\n",
    "Now let us use the FastText embeddings trained on Wiki News corpus in place of Glove embeddings and rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZ2wDCOffyHs",
    "outputId": "61ca0c6d-4a5b-4003-da7e-47d662559cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-16 19:35:08--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  23.7MB/s    in 28s     \n",
      "\n",
      "2020-11-16 19:35:37 (23.1 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRjbM6kmfyqU",
    "outputId": "51115c99-cd2a-456d-ae17-f251252e15c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "BMDhnJ96fzEU"
   },
   "outputs": [],
   "source": [
    "!rm wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "6f3d0fd28dd2b04eaccb732b96b872e5a223d962",
    "id": "nqJuIQfUcODI"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE2 = 'wiki-news-300d-1M.vec'\n",
    "def get_coefs(word,*arr): \n",
    "  return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index2 = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE2) if len(o)>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oA9CsI7fI3j",
    "outputId": "b1339c23-8a0d-45a7-ae39-e26857e7d2a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "all_embs2 = np.stack(embeddings_index2.values())\n",
    "emb_mean2,emb_std2 = all_embs2.mean(), all_embs2.std()\n",
    "embed_size2 = all_embs2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "38CPOOTDfXs4"
   },
   "outputs": [],
   "source": [
    "del all_embs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Bn1EbuB0fJJo"
   },
   "outputs": [],
   "source": [
    "word_index2 = tokenizer.word_index\n",
    "nb_words2 = min(max_features, len(word_index2))\n",
    "embedding_matrix2 = np.random.normal(emb_mean2, emb_std2, (nb_words2, embed_size2))\n",
    "for word, i in word_index2.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector2 = embeddings_index2.get(word)\n",
    "    if embedding_vector2 is not None: embedding_matrix2[i] = embedding_vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "QN1ahyRJfJr9"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size2, weights=[embedding_matrix2])(inp)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "47238831a4701c8a67dc7ecb130ac1402baf7bb2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3hhTJtXcODl",
    "outputId": "60e7a30d-9304-4902-8e53-26c2e73f9c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1786/1786 [==============================] - 386s 216ms/step - loss: 0.1195 - accuracy: 0.9526 - val_loss: 0.1043 - val_accuracy: 0.9582\n",
      "Epoch 2/2\n",
      "1786/1786 [==============================] - 387s 216ms/step - loss: 0.0944 - accuracy: 0.9623 - val_loss: 0.1053 - val_accuracy: 0.9588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7548d15860>"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "b7ab4100f723ad535528865b1edc7896bce80223",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2FuqwgRcOEA",
    "outputId": "27c5bc9d-88c0-43a9-d323-db7dffcc6eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - 16s 43ms/step\n",
      "F1 score at threshold 0.1 is 0.6093761557465568\n",
      "F1 score at threshold 0.11 is 0.6158884428480879\n",
      "F1 score at threshold 0.12 is 0.6212652157875322\n",
      "F1 score at threshold 0.13 is 0.6264502611251838\n",
      "F1 score at threshold 0.14 is 0.6309121219337938\n",
      "F1 score at threshold 0.15 is 0.6347820483907343\n",
      "F1 score at threshold 0.16 is 0.6382846691488666\n",
      "F1 score at threshold 0.17 is 0.6417757164154381\n",
      "F1 score at threshold 0.18 is 0.6441149833965049\n",
      "F1 score at threshold 0.19 is 0.6462729386999848\n",
      "F1 score at threshold 0.2 is 0.6487842598161355\n",
      "F1 score at threshold 0.21 is 0.650946021146355\n",
      "F1 score at threshold 0.22 is 0.6526441673554809\n",
      "F1 score at threshold 0.23 is 0.6538741975067134\n",
      "F1 score at threshold 0.24 is 0.6554183961331823\n",
      "F1 score at threshold 0.25 is 0.65735358232815\n",
      "F1 score at threshold 0.26 is 0.6589069011731443\n",
      "F1 score at threshold 0.27 is 0.6601412602191202\n",
      "F1 score at threshold 0.28 is 0.6613159716375745\n",
      "F1 score at threshold 0.29 is 0.6620202478090058\n",
      "F1 score at threshold 0.3 is 0.6636025555449605\n",
      "F1 score at threshold 0.31 is 0.6637272079634943\n",
      "F1 score at threshold 0.32 is 0.6638568959154738\n",
      "F1 score at threshold 0.33 is 0.663751519667438\n",
      "F1 score at threshold 0.34 is 0.6625448348294791\n",
      "F1 score at threshold 0.35 is 0.6611560323432872\n",
      "F1 score at threshold 0.36 is 0.660446706248233\n",
      "F1 score at threshold 0.37 is 0.6590121140433169\n",
      "F1 score at threshold 0.38 is 0.6579797855040243\n",
      "F1 score at threshold 0.39 is 0.6567759630968707\n",
      "F1 score at threshold 0.4 is 0.6557315154693235\n",
      "F1 score at threshold 0.41 is 0.6542206417746169\n",
      "F1 score at threshold 0.42 is 0.6513020053876085\n",
      "F1 score at threshold 0.43 is 0.6494940996267771\n",
      "F1 score at threshold 0.44 is 0.6475033209207116\n",
      "F1 score at threshold 0.45 is 0.643634684318167\n",
      "F1 score at threshold 0.46 is 0.6399413346370081\n",
      "F1 score at threshold 0.47 is 0.6369020909988334\n",
      "F1 score at threshold 0.48 is 0.6337661160582784\n",
      "F1 score at threshold 0.49 is 0.6308273257809818\n",
      "F1 score at threshold 0.5 is 0.6270649938772209\n"
     ]
    }
   ],
   "source": [
    "pred_fasttext_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_fasttext_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "tKFj3lSgJbAd"
   },
   "outputs": [],
   "source": [
    "del word_index2, embeddings_index2,  embedding_matrix2, model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1312b7a4c3b67ca4ebd26fb083dbac3b6635dc2",
    "id": "KYloa4DMcOIQ"
   },
   "source": [
    "**Observations:**\n",
    " * Overall pretrained embeddings seem to give better results comapred to non-pretrained model. \n",
    " * The performance of the different pretrained embeddings are almost similar.\n",
    " \n",
    "**Final Blend:**\n",
    "\n",
    "Though the results of the models with different pre-trained embeddings are similar, there is a good chance that they might capture different type of information from the data. So let us do a blend of these three models by averaging their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "449bc59fdc9a719aa0759ac51a4481df113604ca",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HX-1pbblcOIT",
    "outputId": "fab0a674-7545-495c-8cd0-64fd862e396a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.5969944973604089\n",
      "F1 score at threshold 0.11 is 0.6055569833518947\n",
      "F1 score at threshold 0.12 is 0.612854789568057\n",
      "F1 score at threshold 0.13 is 0.6190942545109211\n",
      "F1 score at threshold 0.14 is 0.6245151162966206\n",
      "F1 score at threshold 0.15 is 0.6303167837100186\n",
      "F1 score at threshold 0.16 is 0.6350749291211016\n",
      "F1 score at threshold 0.17 is 0.6394815868500079\n",
      "F1 score at threshold 0.18 is 0.6441215577591594\n",
      "F1 score at threshold 0.19 is 0.6472220869649901\n",
      "F1 score at threshold 0.2 is 0.6502030548659181\n",
      "F1 score at threshold 0.21 is 0.6536317497295948\n",
      "F1 score at threshold 0.22 is 0.6565445731214651\n",
      "F1 score at threshold 0.23 is 0.6589829585113808\n",
      "F1 score at threshold 0.24 is 0.660456482881892\n",
      "F1 score at threshold 0.25 is 0.6629818889700003\n",
      "F1 score at threshold 0.26 is 0.6646641371557055\n",
      "F1 score at threshold 0.27 is 0.6664774742816602\n",
      "F1 score at threshold 0.28 is 0.6679922613929492\n",
      "F1 score at threshold 0.29 is 0.6694998010777966\n",
      "F1 score at threshold 0.3 is 0.6705968242379996\n",
      "F1 score at threshold 0.31 is 0.672121178616839\n",
      "F1 score at threshold 0.32 is 0.6728610718798781\n",
      "F1 score at threshold 0.33 is 0.6736506746626686\n",
      "F1 score at threshold 0.34 is 0.6743697876283591\n",
      "F1 score at threshold 0.35 is 0.6746114977595575\n",
      "F1 score at threshold 0.36 is 0.6747548548356085\n",
      "F1 score at threshold 0.37 is 0.6747581286232235\n",
      "F1 score at threshold 0.38 is 0.6743563175695491\n",
      "F1 score at threshold 0.39 is 0.6746014542731591\n",
      "F1 score at threshold 0.4 is 0.6744435612082671\n",
      "F1 score at threshold 0.41 is 0.6730484239195739\n",
      "F1 score at threshold 0.42 is 0.6723198061780739\n",
      "F1 score at threshold 0.43 is 0.6710100372579758\n",
      "F1 score at threshold 0.44 is 0.6704046005339905\n",
      "F1 score at threshold 0.45 is 0.669112313208564\n",
      "F1 score at threshold 0.46 is 0.6675307841866494\n",
      "F1 score at threshold 0.47 is 0.6654158321914198\n",
      "F1 score at threshold 0.48 is 0.6646524394136045\n",
      "F1 score at threshold 0.49 is 0.6626459478021979\n",
      "F1 score at threshold 0.5 is 0.6604467919300372\n"
     ]
    }
   ],
   "source": [
    "pred_val_y = 0.70*pred_glove_val_y + 0.30*pred_fasttext_val_y \n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "a_look_at_different_embeddings_naveen.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
